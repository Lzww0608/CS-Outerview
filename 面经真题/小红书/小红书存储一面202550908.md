# 小红书存储一面202550908



IO子系统：

## 介绍一下 pwrite 的整个流程

`pwrite` 的整个流程核心在于**内核提供了一个将“定位”和“写入”捆绑在一起的原子操作**。这个设计巧妙地避免了在并发环境中因共享文件偏移量而产生的竞态条件，从而实现了高效且线程安全的随机文件写入。它通过将偏移量作为参数直接传入，绕过了对共享文件状态（文件偏移量）的修改，是构建高性能并发 I/O 程序的利器。



## 顺序读场景下 direct io 和 buffer io 性能对比分析

在**顺序读**场景下：

- 对于绝大多数普通应用程序（如 `cat`, `grep`, `cp`，或者一般的业务逻辑文件处理），**Buffered I/O 是更好、更快、更简单的选择**。内核的 Readahead 优化在顺序读场景下表现极为出色，几乎总是能带来最佳性能。
- 只有在你明确知道自己在做什么，并且符合以下一个或多个条件时，才应该考虑使用 **Direct I/O**：
  1. 你的应用程序需要读取一个巨大无比的文件（例如 > 总物理内存），并且你不希望这次操作对系统上其他正在运行的服务造成缓存性能冲击。
  2. 你的应用程序（通常是数据库、虚拟化或存储系统）在用户空间实现了自己的、比内核更高级的缓存管理机制。
  3. 你的硬件 I/O 速度快到极致 (如高性能 NVMe RAID)，以至于 CPU 的内存拷贝成为了新的性能瓶颈。
  4. 你需要精确控制 I/O 的时序和数据落盘，避免内核的延迟和不确定性。



## page cache 的 prefetch 机制有了解吗





page cache 的大小是由谁决定的

数据拷贝过程，从磁盘到 cpu 的 l1 cache

异步 io 用过吗

同步 io 的情况下，cpu 的使用情况，介绍一下 DMA



## io_uring 相比 aio 性能好在哪里，主要是哪方面减少了开销



## io_uring 的 polling 模式和 spdk 比较





说一下项目中的性能测试部分，SSD 中的 cache 会对性能产生影响吗，这一块是怎么处理的

存储引擎：
介绍一下 B+Tree 和 LSM-Tree

B+Tree 并发控制，介绍一下 latch_crabbing

二者各自的读写放大情况

LSM-Tree 的 delete 操作，墓碑什么时候能被删除

大量的 delete 对性能的影响，解决方案

## 说一下 LSM-Tree 的 KV 分离





## KV 分离的负面影响，有什么解决方案





## KV 分离导致 Scan 性能下降的问题，目前学术界有什么解决的方案





## RocksDB 的 WriteBatch，如何保证原子性，项目当中又是如何保证原子性的





## RocksDB 的一致性读，memtable 当中如何保证一致性读





分布式：
Raft 大论文中的 PreVote
Raft 需要持久化的信息，voteFor 丢失会怎么样
Joint consensus
c++
std::move，给了一个场景题
c++ share_ptr 的线程安全是怎么实现的
介绍一下内存序都有哪些，对于 release-acquire，happens-before 是如何建立的
算法：二分搜索数的范围